<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lhuhi - Advanced Assistant (V4)</title>
    
    <style>
        body { background-color: #000; margin: 0; display: flex; justify-content: center; align-items: center; height: 100vh; overflow: hidden; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif; color: #ccc; }
        .circle { width: 200px; height: 200px; background: radial-gradient(circle, #007bff, #0056b3); border-radius: 50%; transition: all 0.3s ease-in-out; box-shadow: 0 0 20px #007bff, 0 0 40px #007bff, 0 0 60px #007bff; }
        .circle.active { transform: scale(1.2); animation: pulse 1.5s infinite; }
        @keyframes pulse { 0% { box-shadow: 0 0 25px #007bff, 0 0 50px #007bff; } 50% { box-shadow: 0 0 50px #00aaff, 0 0 100px #00aaff; } 100% { box-shadow: 0 0 25px #007bff, 0 0 50px #007bff; } }
        .container { position: absolute; text-align: center; }
        .mic-container { bottom: 40px; }
        #mic-button { background-color: #222; border: 3px solid #555; border-radius: 50%; padding: 20px; cursor: pointer; transition: all 0.2s; }
        #mic-button.listening { border-color: #00ff7f; transform: scale(1.1); }
        #mic-icon { width: 50px; height: 50px; display: block; filter: invert(1); }
        #status-text { top: 40px; font-size: 1.2em; transition: opacity 0.3s; }
    </style>
</head>
<body>

    <div class="container" id="status-text">Mic button dabakar baat karein...</div>
    <div class="circle" id="voice-circle"></div>
    <div class="container mic-container">
        <button id="mic-button">
            <img id="mic-icon" src="https://img.icons8.com/ios-filled/100/microphone.png" alt="Microphone">
        </button>
    </div>

    <script>
        // --- ‚öôÔ∏è CONFIGURATION & API KEYS ‚öôÔ∏è ---
        // AAPKE KEHNE PAR YE KEYS USE KI GAYI HAIN. PLEASE INHE TURANT BADAL LEIN.
        const GEMINI_API_KEY = 'AIzaSyDeAWuz0GxiaOuF1txKA6jaoj0yplsIAns';
        const ELEVENLABS_API_KEY = 'sk_32575f00d6c61366f00a114698993270362892674d15d9de';
        const ELEVENLABS_VOICE_ID = 'amiAXapsDOAiHJqbsAZj';

        // --- DOM & STATE MANAGEMENT ---
        const micButton = document.getElementById('mic-button');
        const voiceCircle = document.getElementById('voice-circle');
        const statusText = document.getElementById('status-text');
        
        let recognition;
        let isListening = false;
        let currentAudio = null; // <<< To control the AI's speech

        // <<< CONVERSATION HISTORY ARRAY >>>
        let conversationHistory = [];

        // --- üé§ SPEECH RECOGNITION (INPUT) üé§ ---
        if ('webkitSpeechRecognition' in window) {
            recognition = new webkitSpeechRecognition();
            recognition.continuous = false; // Sirf ek baar sunega
            recognition.interimResults = false;
            recognition.lang = 'hi-IN';

            recognition.onstart = () => {
                isListening = true;
                micButton.classList.add('listening');
                statusText.textContent = "Sun raha hun...";
                console.log("üé§ Listening started...");
            };

            recognition.onresult = (event) => {
                const query = event.results[0][0].transcript;
                console.log("‚úÖ User said:", query);
                // Mic off karke hi aage badho
                stopListeningAndProcess(query);
            };
            
            recognition.onerror = (event) => {
                console.error('üî¥ Recognition error:', event.error);
                resetState();
            };
            
            recognition.onend = () => {
                isListening = false;
                micButton.classList.remove('listening');
                // Agar AI bolna shuru na kare, to status reset karo
                if (!currentAudio) {
                    statusText.textContent = "Mic button dabakar baat karein...";
                }
            };
        } else { alert('Sorry, your browser does not support voice recognition.'); }

        // --- üïπÔ∏è MAIN CONTROL LOGIC üïπÔ∏è ---
        micButton.addEventListener('click', () => {
            // <<< INTERRUPT FEATURE >>>
            if (currentAudio) {
                console.log("üö´ Interrupting AI speech.");
                currentAudio.pause();
                currentAudio = null;
                resetState();
            }

            if (!isListening) {
                recognition.start();
            } else {
                recognition.stop();
            }
        });
        
        function stopListeningAndProcess(query) {
             if (isListening) {
                recognition.stop();
             }
             statusText.textContent = "Soch raha hun...";
             analyzeWithGemini(query);
        }
        
        function resetState() {
            isListening = false;
            micButton.classList.remove('listening');
            voiceCircle.classList.remove('active');
            statusText.textContent = "Mic button dabakar baat karein...";
        }

        // --- üß† GEMINI AI (WITH MEMORY) üß† ---
        async function analyzeWithGemini(userQuery) {
            voiceCircle.classList.add('active');

            // Add user's new query to history
            conversationHistory.push({ role: 'user', parts: [{ text: userQuery }] });

            const API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=${GEMINI_API_KEY}`;
            
            console.log("üß† Sending to Gemini with history:", conversationHistory);
            try {
                const response = await fetch(API_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ contents: conversationHistory }), // <<< SENDING FULL HISTORY
                });

                if (!response.ok) throw new Error(`Gemini API Error: ${response.statusText}`);
                
                const result = await response.json();
                if (!result.candidates || result.candidates.length === 0) {
                    throw new Error("Gemini returned no response.");
                }

                const geminiResponseText = result.candidates[0].content.parts[0].text;
                
                // Add model's response to history
                conversationHistory.push({ role: 'model', parts: [{ text: geminiResponseText }] });
                
                console.log("ü§ñ Gemini responded:", geminiResponseText);
                await speakWithElevenLabs(geminiResponseText);

            } catch (error) {
                console.error("üî¥ FATAL ERROR with Gemini:", error);
                statusText.textContent = "Maaf kijiye, aage sampark nahi ho paa raha hai.";
                voiceCircle.classList.remove('active');
            }
        }

        // --- üîä ELEVENLABS TTS (VOICE OUTPUT) üîä ---
        async function speakWithElevenLabs(text) {
            statusText.textContent = "Bol raha hun...";

            const API_URL = `https://api.elevenlabs.io/v1/text-to-speech/${ELEVENLABS_VOICE_ID}`;
            
            try {
                const response = await fetch(API_URL, {
                    method: 'POST',
                    headers: { 'Accept': 'audio/mpeg', 'Content-Type': 'application/json', 'xi-api-key': ELEVENLABS_API_KEY },
                    body: JSON.stringify({
                        text: text, model_id: 'eleven_multilingual_v2',
                        voice_settings: { stability: 0.55, similarity_boost: 0.75 },
                    }),
                });

                if (!response.ok) throw new Error(`ElevenLabs API Error: ${response.statusText}`);

                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);
                currentAudio = new Audio(audioUrl); // <<< Store audio object globally
                currentAudio.play();

                currentAudio.onended = () => {
                    console.log("‚úÖ Speech finished.");
                    currentAudio = null;
                    resetState();
                };

            } catch (error) {
                console.error("üî¥ FATAL ERROR with ElevenLabs:", error);
                currentAudio = null;
                resetState();
            }
        }
    </script>
</body>
</html>
