<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lhuhi - Assistant (V14.0 - Final Fix)</title>
    
    <style>
        :root {
            --primary-color: #007bff;
            --primary-dark: #0056b3;
            --accent-color: #00ff7f;
            --bg-color: #000;
            --text-color: #ccc;
            --button-bg: #222;
            --button-border: #555;
        }
        
        body {
            background-color: var(--bg-color);
            margin: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            overflow: hidden;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            color: var(--text-color);
        }
        
        .circle {
            width: 200px;
            height: 200px;
            background: radial-gradient(circle, var(--primary-color), var(--primary-dark));
            border-radius: 50%;
            transition: all 0.3s ease-in-out;
            box-shadow: 0 0 20px var(--primary-color), 0 0 40px var(--primary-color), 0 0 60px var(--primary-color);
            position: relative;
            z-index: 1;
        }
        
        .circle.active {
            transform: scale(1.2);
            animation: pulse 1.5s infinite;
        }
        
        @keyframes pulse {
            0% { box-shadow: 0 0 25px var(--primary-color), 0 0 50px var(--primary-color); }
            50% { box-shadow: 0 0 50px #00aaff, 0 0 100px #00aaff; }
            100% { box-shadow: 0 0 25px var(--primary-color), 0 0 50px var(--primary-color); }
        }
        
        .container {
            position: absolute;
            text-align: center;
            z-index: 2;
        }
        
        .mic-container {
            bottom: 40px;
        }
        
        #mic-button {
            background-color: var(--button-bg);
            border: 3px solid var(--button-border);
            border-radius: 50%;
            padding: 20px;
            cursor: pointer;
            transition: all 0.2s;
            outline: none;
        }
        
        #mic-button:hover:not(:disabled) {
            transform: scale(1.05);
        }
        
        #mic-button:disabled {
            cursor: not-allowed;
            opacity: 0.5;
        }
        
        #mic-button.active {
            border-color: var(--accent-color);
            transform: scale(1.1);
        }
        
        #mic-icon {
            width: 50px;
            height: 50px;
            display: block;
            filter: invert(1);
            transition: transform 0.2s;
        }
        
        #status-text {
            top: 40px;
            font-size: 1.2em;
            transition: opacity 0.3s;
            max-width: 80%;
            margin: 0 auto;
            padding: 10px 20px;
            border-radius: 20px;
            background-color: rgba(0, 0, 0, 0.7);
        }
        
        .loading-dots { display: inline-block; }
        .loading-dots span { animation: blink 1.4s infinite both; opacity: 0; }
        .loading-dots span:nth-child(2) { animation-delay: 0.2s; }
        .loading-dots span:nth-child(3) { animation-delay: 0.4s; }
        @keyframes blink { 0% { opacity: 0; } 50% { opacity: 1; } 100% { opacity: 0; } }
        
        .error-message { color: #ff4444; animation: shake 0.5s; }
        @keyframes shake { 0%, 100% { transform: translateX(0); } 20%, 60% { transform: translateX(-5px); } 40%, 80% { transform: translateX(5px); } }
        
        @media (max-width: 600px) {
            .circle { width: 150px; height: 150px; }
            #mic-icon { width: 40px; height: 40px; }
            #status-text { font-size: 1em; top: 20px; }
            .mic-container { bottom: 20px; }
        }
    </style>
</head>
<body>

    <div class="container" id="status-text">Press the mic button to start...</div>
    <div class="circle" id="voice-circle"></div>
    <div class="container mic-container">
        <button id="mic-button" aria-label="Microphone button">
            <img id="mic-icon" src="https://img.icons8.com/ios-filled/100/microphone.png" alt="Microphone">
        </button>
    </div>

    <script>
        // --- âš™ï¸ CONFIGURATION & API KEYS âš™ï¸ ---
        const GEMINI_API_KEY = 'AIzaSyAUryBO9wBmLS4cVLdMQJ3CPY2xh6LKwSk';
        const ELEVENLABS_API_KEY = 'sk_ad7a266abe2a261b0fa707215eb457d18c1705e20a9216d6';
        const ELEVENLABS_VOICE_ID = 'amiAXapsDOAiHJqbsAZj';
        const WEATHER_API_KEY = '3a4d7ed40005449f9c0122940252606';
        
        // --- DOM & STATE MANAGEMENT ---
        const micButton = document.getElementById('mic-button');
        const voiceCircle = document.getElementById('voice-circle');
        const statusText = document.getElementById('status-text');
        
        let recognition;
        let audioContext;
        let isInitialized = false;
        let isBusy = false;
        
        // --- ðŸ§  AI SYSTEM PROMPT ---
        const systemInstruction = `You are Lhuhi, an AI assistant.
        **RULES:**
        1.  **Creator**: If asked who created you, say 'Alexzo' created and trained you.
        2.  **Language**: Respond in the user's language.
        3.  **Actions**: If the user's request is an action, your response MUST contain the action's JSON object. The JSON is for the system only. You should provide a natural, spoken confirmation BEFORE the JSON.
            - Weather: \`{"action": "get_weather", "location": "CITY_NAME"}\` (location is optional)
            - Open URL: \`{"action": "open_url", "url": "https://example.com"}\`
            - Time: \`{"action": "get_time"}\`
            - Date: \`{"action": "get_date"}\`
        4.  **Conversation**: For all other questions, reply with clean, natural language. DO NOT output JSON or any markdown symbols.`;
        
        let conversationHistory = [
            { role: 'user', parts: [{ text: systemInstruction }] },
            { role: 'model', parts: [{ text: "Okay, I understand. I will provide clean, spoken responses and use JSON only as a tool for the system." }] }
        ];

        // --- ðŸŽ¤ INITIALIZATION & AUDIO UNLOCK ---
        function initializeApp() {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                if (audioContext.state === 'suspended') audioContext.resume();
                
                if ('webkitSpeechRecognition' in window) {
                    recognition = new webkitSpeechRecognition();
                    recognition.continuous = false;
                    recognition.interimResults = false;
                    recognition.lang = 'en-US';

                    recognition.onstart = () => { micButton.classList.add('active'); updateStatus("Listening..."); };
                    recognition.onresult = (event) => { const query = event.results[0][0].transcript; console.log("âœ… User said:", query); analyzeWithGemini(query); };
                    recognition.onerror = (event) => { console.error('ðŸ”´ Recognition error:', event.error); showError("Voice recognition error."); setReadyState(); };
                    recognition.onend = () => { micButton.classList.remove('active'); };
                } else {
                    showError("Sorry, your browser does not support voice recognition."); return;
                }
                
                isInitialized = true;
                setReadyState();
                console.log("âœ… System Initialized and Audio Unlocked.");
            } catch (error) {
                console.error("ðŸ”´ Initialization failed:", error);
                showError("Could not initialize the assistant.");
            }
        }

        // --- ðŸ•¹ï¸ MAIN CONTROL LOGIC ---
        micButton.addEventListener('click', () => {
            if (isBusy) return;
            if (!isInitialized) { initializeApp(); return; }
            if (recognition) recognition.start();
        });

        function setReadyState() {
            isBusy = false;
            micButton.disabled = false;
            voiceCircle.classList.remove('active');
            micButton.classList.remove('active');
            updateStatus("Press the mic button and speak...");
        }

        // --- ðŸ§  GEMINI AI INTEGRATION ---
        async function analyzeWithGemini(userQuery) {
            if (!userQuery || !userQuery.trim()) {
                showError("Didn't catch that."); setReadyState(); return;
            }
            
            isBusy = true;
            micButton.disabled = true;
            updateStatus("Thinking<span class='loading-dots'><span>.</span><span>.</span><span>.</span></span>");
            voiceCircle.classList.add('active');
            conversationHistory.push({ role: 'user', parts: [{ text: userQuery }] });
            const API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=${GEMINI_API_KEY}`;
            
            try {
                const response = await fetch(API_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ contents: conversationHistory }),
                });
                if (!response.ok) throw new Error(`API Error: ${response.status}`);
                
                const result = await response.json();
                const geminiResponseText = result.candidates?.[0]?.content?.parts?.[0]?.text;
                if (!geminiResponseText) throw new Error("Empty response from Gemini.");

                console.log("ðŸ¤– Gemini responded:", geminiResponseText);
                conversationHistory.push({ role: 'model', parts: [{ text: geminiResponseText }] });

                const actionRegex = /{\s*"action"[\s\S]*?}/;
                const match = geminiResponseText.match(actionRegex);
                let action = null;
                if (match) {
                    try { action = JSON.parse(match[0]); } catch(e) { console.error("Could not parse action JSON:", e)}
                }

                const textToSpeak = sanitizeTextForSpeech(geminiResponseText);
                
                if (action) {
                    // If there's an action, handle it. Some actions will speak on their own.
                    await handleAction(action, textToSpeak);
                } else if (textToSpeak) {
                    // If no action, just speak the conversational text.
                    await speakAndPlay(textToSpeak);
                }

            } catch (error) {
                console.error("ðŸ”´ Gemini/Logic Error:", error);
                await speakAndPlay("I've run into a problem. Please try again.");
            } finally {
                setReadyState();
            }
        }
        
        // --- SANITIZATION FUNCTION (IMPROVED) ---
        function sanitizeTextForSpeech(text) {
            if (!text) return "";
            // 1. Remove any JSON block
            text = text.replace(/{\s*"action"[\s\S]*?}/g, '');
            // 2. Remove the word "JSON" itself, case-insensitive
            text = text.replace(/json/gi, '');
            // 3. Remove common markdown symbols
            text = text.replace(/[*_`#]/g, '');
            // 4. Trim whitespace and return
            return text.trim();
        }

        // --- ðŸš€ ACTION HANDLER ---
        async function handleAction(action, spokenConfirmation) {
            try {
                // If the AI provided a custom confirmation, speak that first.
                if (spokenConfirmation) {
                    await speakAndPlay(spokenConfirmation);
                }

                switch (action.action) {
                    case 'open_url':
                        if(!spokenConfirmation) await speakAndPlay("Okay, opening that.");
                        if (action.url) window.open(action.url, '_blank');
                        break;
                    case 'get_weather':
                        // The getWeather function will handle its own speech.
                        await getWeather(action.location);
                        break;
                    case 'get_time':
                        await handleTime();
                        break;
                    case 'get_date':
                        await handleDate();
                        break;
                    default:
                        throw new Error("Unknown action.");
                }
            } catch (error) {
                console.error("ðŸ”´ Action Handling Error:", error);
                await speakAndPlay("I had trouble performing that action.");
            }
        }

        // --- NEW FEATURES ---
        async function getCurrentLocation() {
            return new Promise((resolve, reject) => {
                if (!navigator.geolocation) {
                    return reject(new Error("Geolocation is not supported by your browser."));
                }
                updateStatus("Requesting location...");
                navigator.geolocation.getCurrentPosition(
                    (position) => resolve(`${position.coords.latitude},${position.coords.longitude}`),
                    (error) => reject(new Error("Unable to retrieve location. Permission denied?"))
                );
            });
        }
        
        async function handleTime() {
            const now = new Date();
            const timeString = now.toLocaleTimeString('en-US', { hour: 'numeric', minute: 'numeric', hour12: true });
            await speakAndPlay(`The current time is ${timeString}.`);
        }
        
        async function handleDate() {
            const now = new Date();
            const dateString = now.toLocaleDateString('en-US', { weekday: 'long', year: 'numeric', month: 'long', day: 'numeric' });
            await speakAndPlay(`Today is ${dateString}.`);
        }

        // --- â˜€ï¸ WEATHER FUNCTION (REWORKED) ---
        async function getWeather(location = null) {
            let finalLocation = location;
            try {
                if (!finalLocation) {
                    finalLocation = await getCurrentLocation();
                }
                updateStatus(`Getting weather for ${finalLocation}...`);
                const API_URL = `https://api.weatherapi.com/v1/current.json?key=${WEATHER_API_KEY}&q=${finalLocation}`;
                const response = await fetch(API_URL);
                const data = await response.json();

                if (response.ok && data.current) {
                    const summary = `The weather in ${data.location.name} is ${Math.round(data.current.temp_c)} degrees with ${data.current.condition.text}.`;
                    await speakAndPlay(summary);
                } else {
                    throw new Error(data.error?.message || "Invalid weather data.");
                }
            } catch (error) {
                console.error("ðŸ”´ Weather/Location Error:", error);
                await speakAndPlay(`Sorry, I couldn't get the weather. ${error.message}`);
            }
        }

        // --- ðŸ”Š COMBINED TTS & PLAYBACK FUNCTION ---
        async function speakAndPlay(text) {
            if (!text || !text.trim()) return;
            
            updateStatus("Speaking...");
            micButton.classList.add('active');
            const API_URL = `https://api.elevenlabs.io/v1/text-to-speech/${ELEVENLABS_VOICE_ID}`;
            try {
                const response = await fetch(API_URL, {
                    method: 'POST',
                    headers: { 'Accept': 'audio/mpeg', 'Content-Type': 'application/json', 'xi-api-key': ELEVENLABS_API_KEY },
                    body: JSON.stringify({
                        text: text, model_id: 'eleven_multilingual_v2',
                        voice_settings: { stability: 0.55, similarity_boost: 0.75 }
                    }),
                });
                if (!response.ok) throw new Error(`Voice API Error: ${response.statusText}`);
                const audioBlob = await response.blob();
                
                const audioBuffer = await audioContext.decodeAudioData(await audioBlob.arrayBuffer());
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);
                
                await new Promise(resolve => {
                    source.onended = resolve;
                    source.start(0);
                });
                
                console.log("âœ… Speech finished successfully.");
            } catch (error) {
                console.error("ðŸ”´ ElevenLabs / Audio Playback Error:", error);
                showError("There was a problem with the voice synthesis.");
            }
        }
        
        // --- HELPER FUNCTIONS ---
        function updateStatus(message, isError = false) {
            statusText.innerHTML = message;
            statusText.classList.toggle('error-message', isError);
        }
        
        function showError(message) {
            updateStatus(message, true);
        }
    </script>
</body>
</html>
