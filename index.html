<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lhuhi - Hyper-Realistic 3D Assistant</title>
    
    <style>
        :root {
            --primary-color: #00AFFF; /* Electric Blue */
            --accent-color: #00FF7F; /* Spring Green */
            --glow-color: rgba(0, 175, 255, 0.7);
            --bg-color: #000;
            --text-color: #E0E0E0;
            --button-bg: rgba(34, 34, 34, 0.5);
        }
        
        body {
            background-color: var(--bg-color);
            margin: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            overflow: hidden;
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            color: var(--text-color);
            perspective: 1000px; /* Essential for 3D transforms */
        }
        
        /* Main container for the 3D sphere */
        #voice-sphere-container {
            position: relative;
            width: 300px;
            height: 300px;
            transform-style: preserve-3d;
            animation: idle-breathe 8s ease-in-out infinite;
        }

        #voice-sphere {
            position: absolute;
            width: 100%;
            height: 100%;
            transform-style: preserve-3d;
            transition: transform 0.5s ease-out;
        }
        
        /* The rings that make up the sphere */
        .ring {
            position: absolute;
            top: 0; left: 0;
            width: 100%;
            height: 100%;
            border-radius: 50%;
            border: 2px solid var(--primary-color);
            box-shadow: 0 0 10px var(--glow-color), inset 0 0 10px var(--glow-color);
            background: radial-gradient(circle, rgba(0, 175, 255, 0.1) 0%, rgba(0, 175, 255, 0) 60%);
            transition: all 0.5s ease-out;
        }

        /* Arranging rings in a 3D sphere shape */
        .ring:nth-child(1) { transform: rotateY(0deg); }
        .ring:nth-child(2) { transform: rotateY(30deg); }
        .ring:nth-child(3) { transform: rotateY(60deg); }
        .ring:nth-child(4) { transform: rotateY(90deg); }
        .ring:nth-child(5) { transform: rotateY(120deg); }
        .ring:nth-child(6) { transform: rotateY(150deg); }
        
        /* --- STATE-BASED ANIMATIONS --- */
        @keyframes idle-breathe {
            0%, 100% { transform: rotateX(15deg) rotateY(0deg) scale(1); }
            50% { transform: rotateX(10deg) rotateY(20deg) scale(1.02); }
        }

        @keyframes think-spin {
            from { transform: rotateY(0deg) rotateX(20deg); }
            to { transform: rotateY(360deg) rotateX(20deg); }
        }
        
        @keyframes speak-pulse {
            0%, 100% { transform: scale3d(1, 1, 1); }
            50% { transform: scale3d(1.05, 1.05, 1.05); }
        }

        /* Applying animations based on class */
        #voice-sphere-container.thinking #voice-sphere {
            animation: think-spin 1.2s linear infinite;
        }

        #voice-sphere-container.speaking #voice-sphere {
            animation: speak-pulse 0.8s ease-in-out infinite;
        }

        #voice-sphere-container.listening .ring {
            border-color: var(--accent-color);
            box-shadow: 0 0 15px var(--accent-color), inset 0 0 15px var(--accent-color);
            transform: scale(1.1) !important; /* Force scale on listening */
        }

        /* UI elements */
        #status-text {
            position: absolute;
            top: 20%;
            font-size: 1.5em;
            font-weight: 300;
            text-shadow: 0 0 10px var(--primary-color);
            padding: 10px 25px;
            border-radius: 30px;
            background: rgba(0, 0, 0, 0.3);
            backdrop-filter: blur(5px);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        #mic-button {
            position: absolute;
            bottom: 60px;
            background-color: var(--button-bg);
            border: 2px solid rgba(255, 255, 255, 0.2);
            backdrop-filter: blur(10px);
            border-radius: 50%;
            padding: 20px;
            cursor: pointer;
            transition: all 0.3s;
            outline: none;
        }
        
        #mic-button:hover:not(:disabled) {
            transform: scale(1.1);
            border-color: var(--primary-color);
        }
        
        #mic-button:disabled { cursor: not-allowed; opacity: 0.4; }
        #mic-icon { width: 50px; height: 50px; display: block; filter: invert(0.9); }

        @media (max-width: 600px) {
            #voice-sphere-container { width: 220px; height: 220px; }
            #status-text { font-size: 1.1em; top: 25%; }
        }
    </style>
</head>
<body>

    <div id="status-text">Press the mic to start</div>
    <div id="voice-sphere-container">
        <div id="voice-sphere">
            <div class="ring"></div>
            <div class="ring"></div>
            <div class="ring"></div>
            <div class="ring"></div>
            <div class="ring"></div>
            <div class="ring"></div>
        </div>
    </div>
    <button id="mic-button" aria-label="Microphone button">
        <img id="mic-icon" src="https://img.icons8.com/ios-filled/100/microphone.png" alt="Microphone">
    </button>

    <script>
        // --- âš™ï¸ CONFIGURATION & API KEYS âš™ï¸ ---
        const GEMINI_API_KEY = 'AIzaSyAUryBO9wBmLS4cVLdMQJ3CPY2xh6LKwSk';
        const WEATHER_API_KEY = '3a4d7ed40005449f9c0122940252606';
        
        // --- DOM & STATE MANAGEMENT ---
        const micButton = document.getElementById('mic-button');
        const sphereContainer = document.getElementById('voice-sphere-container');
        const statusText = document.getElementById('status-text');
        
        let recognition;
        let audioContext;
        const synthesis = window.speechSynthesis;
        let preferredVoice = null;
        let isInitialized = false;
        
        // --- ðŸ§  AI SYSTEM PROMPT ---
        const systemInstruction = `You are Lhuhi, an AI assistant.
        **RULES:**
        1.  **Creator**: If asked who created you, say 'Alexzo' created and trained you.
        2.  **Language**: Respond in the user's language.
        3.  **Actions**: If the user's request is an action, your response MUST contain the action's JSON object. You should provide a natural, spoken confirmation BEFORE the JSON. The JSON is for the system only and should NEVER be spoken.
            - For weather OR time in a specific city (e.g., "weather in London", "time in Tokyo"): \`{"action": "get_info_for_city", "location": "CITY_NAME"}\`.
            - To open a URL: \`{"action": "open_url", "url": "https://example.com"}\`.
            - For the user's LOCAL time (e.g., "what's the time?"): \`{"action": "get_local_time"}\`.
            - For the current date: \`{"action": "get_date"}\`.
        4.  **Conversation**: For ALL other questions, reply with clean, natural language. DO NOT output JSON or any markdown symbols.`;
        
        let conversationHistory = [
            { role: 'user', parts: [{ text: systemInstruction }] },
            { role: 'model', parts: [{ text: "Okay, I understand. I will provide clean, spoken responses and use JSON only as a tool for the system." }] }
        ];

        // --- ðŸŽ¤ INITIALIZATION & AUDIO UNLOCK ---
        async function initializeApp() {
            try {
                // Setup Speech Recognition
                if ('webkitSpeechRecognition' in window) {
                    recognition = new webkitSpeechRecognition();
                    recognition.continuous = false;
                    recognition.interimResults = false;
                    recognition.lang = 'en-US';

                    recognition.onstart = () => setUIState('listening');
                    recognition.onresult = (event) => { const query = event.results[0][0].transcript; console.log("âœ… User said:", query); analyzeWithGemini(query); };
                    recognition.onerror = (event) => {
                        console.error('ðŸ”´ Recognition error:', event.error);
                        if (event.error !== 'no-speech') {
                             setUIState('error', "Voice error.");
                        } else {
                             setUIState('idle');
                        }
                    };
                    recognition.onend = () => { if(sphereContainer.classList.contains('listening')) setUIState('idle'); };
                } else {
                    setUIState('error', "Browser not supported.");
                    return;
                }
                
                // Setup Speech Synthesis (The critical fix is here)
                await setupSpeechSynthesis();
                
                isInitialized = true;
                setUIState('idle');
                console.log("âœ… System Initialized and Voices are Ready.");
            } catch (error) {
                console.error("ðŸ”´ Initialization failed:", error);
                setUIState('error', "Could not initialize voice.");
            }
        }
        
        // --- ðŸ”Š ROBUST VOICE LOADING FUNCTION ---
        function setupSpeechSynthesis() {
            return new Promise((resolve, reject) => {
                const loadVoices = () => {
                    const voices = synthesis.getVoices();
                    if (voices.length > 0) {
                        selectVoice(voices);
                        if (preferredVoice) {
                            console.log(`âœ… Voice selected: ${preferredVoice.name} (${preferredVoice.lang})`);
                            resolve();
                        } else {
                            console.warn("âš ï¸ Could not find a preferred female English voice. Using browser default.");
                            resolve(); // Resolve anyway, letting the browser pick a default
                        }
                    }
                };

                if (synthesis.getVoices().length > 0) {
                    loadVoices();
                } else {
                    synthesis.onvoiceschanged = loadVoices;
                    setTimeout(() => {
                        if (!preferredVoice && synthesis.getVoices().length === 0) {
                            reject(new Error("Voice loading timed out."));
                        }
                    }, 3000);
                }
            });
        }

        function selectVoice(voices) {
            const preferredVoiceNames = [
                'Google US English', 'Samantha', 'Microsoft Zira Desktop - English (United States)', 'Google UK English Female',
            ];
            for (const name of preferredVoiceNames) {
                const foundVoice = voices.find(voice => voice.name === name && voice.lang.startsWith('en'));
                if (foundVoice) {
                    preferredVoice = foundVoice;
                    return;
                }
            }
            preferredVoice = voices.find(voice => voice.lang.startsWith('en') && /female/i.test(voice.name)) || voices.find(voice => voice.lang.startsWith('en-US'));
        }

        // --- UI STATE MACHINE ---
        function setUIState(state, message = '') {
            const states = ['idle', 'listening', 'thinking', 'speaking', 'error'];
            states.forEach(s => sphereContainer.classList.remove(s));
            sphereContainer.classList.add(state);
            micButton.disabled = (state === 'thinking' || state === 'speaking');
            
            switch(state) {
                case 'idle': statusText.textContent = message || 'Press the mic to speak'; break;
                case 'listening': statusText.textContent = 'Listening...'; break;
                case 'thinking': statusText.textContent = 'Thinking...'; break;
                case 'speaking': statusText.textContent = 'Speaking...'; break;
                case 'error':
                    statusText.textContent = message || 'An error occurred.';
                    setTimeout(() => setUIState('idle'), 3000); break;
            }
        }

        // --- ðŸ•¹ï¸ MAIN CONTROL LOGIC ---
        micButton.addEventListener('click', async () => {
            if (micButton.disabled) return;

            if (!isInitialized) {
                setUIState('thinking', 'Initializing...');
                if (!audioContext) audioContext = new (window.AudioContext || window.webkitAudioContext)();
                if (audioContext.state === 'suspended') await audioContext.resume();
                
                await initializeApp();
                
                if(isInitialized) {
                    recognition.start();
                }
            } else {
                if (synthesis.speaking) synthesis.cancel();
                recognition.start();
            }
        });

        // --- ðŸ§  GEMINI AI INTEGRATION ---
        async function analyzeWithGemini(userQuery) {
            if (!userQuery || !userQuery.trim()) {
                setUIState('idle'); return;
            }
            
            setUIState('thinking');
            conversationHistory.push({ role: 'user', parts: [{ text: userQuery }] });
            const API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=${GEMINI_API_KEY}`;
            
            try {
                const response = await fetch(API_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ contents: conversationHistory }),
                });
                if (!response.ok) throw new Error(`API Error: ${response.statusText}`);
                
                const result = await response.json();
                const geminiResponseText = result.candidates?.[0]?.content?.parts?.[0]?.text;
                if (!geminiResponseText) throw new Error("Empty response from Gemini.");

                console.log("ðŸ¤– Gemini responded:", geminiResponseText);
                conversationHistory.push({ role: 'model', parts: [{ text: geminiResponseText }] });

                const actionRegex = /{\s*"action"[\s\S]*?}/;
                const match = geminiResponseText.match(actionRegex);
                let action = null;
                if (match) { try { action = JSON.parse(match[0]); } catch(e) { console.error("Could not parse action JSON:", e)} }
                
                const textToSpeak = sanitizeTextForSpeech(geminiResponseText);
                
                if (action) {
                    await handleAction(action, userQuery, textToSpeak);
                } else if (textToSpeak) {
                    await speakAndPlay(textToSpeak);
                } else {
                    setUIState('idle');
                }

            } catch (error) {
                console.error("ðŸ”´ Gemini/Logic Error:", error);
                await speakAndPlay("I've run into a problem. Please try again.");
            }
        }
        
        function sanitizeTextForSpeech(text) {
            if (!text) return "";
            return text.replace(/{\s*"action"[\s\S]*?}/g, '').replace(/[*_`#]/g, '').trim();
        }

        // --- ðŸš€ ACTION HANDLER ---
        async function handleAction(action, userQuery, spokenConfirmation) {
            try {
                if (spokenConfirmation) await speakAndPlay(spokenConfirmation);
                
                switch (action.action) {
                    case 'open_url':
                        if(!spokenConfirmation) await speakAndPlay("Okay, opening that.");
                        if (action.url) window.open(action.url, '_blank');
                        break;
                    case 'get_info_for_city':
                        const purpose = userQuery.toLowerCase().includes('time') ? 'time' : 'weather';
                        await getInfoForCity(action.location, purpose);
                        break;
                    case 'get_local_time': await handleLocalTime(); break;
                    case 'get_date': await handleDate(); break;
                    default: throw new Error("Unknown action.");
                }
            } catch (error) {
                console.error("ðŸ”´ Action Handling Error:", error);
                await speakAndPlay("I had trouble performing that action.");
            }
        }

        // --- FEATURES ---
        function getCurrentLocation() {
            return new Promise((resolve, reject) => {
                if (!navigator.geolocation) return reject(new Error("Geolocation is not supported."));
                setUIState('thinking', "Requesting location...");
                navigator.geolocation.getCurrentPosition(resolve, reject, { timeout: 10000 });
            });
        }
        
        async function handleLocalTime() {
            const now = new Date();
            const timeString = now.toLocaleTimeString('en-US', { hour: 'numeric', minute: 'numeric', hour12: true });
            await speakAndPlay(`The current time is ${timeString}.`);
        }
        
        async function handleDate() {
            const now = new Date();
            const dateString = now.toLocaleDateString('en-US', { weekday: 'long', year: 'numeric', month: 'long', day: 'numeric' });
            await speakAndPlay(`Today is ${dateString}.`);
        }

        async function getInfoForCity(location = null, purpose = 'weather') {
            let finalLocation = location;
            try {
                if (!finalLocation) {
                    const position = await getCurrentLocation();
                    finalLocation = `${position.coords.latitude},${position.coords.longitude}`;
                }
                setUIState('thinking', `Getting info for ${finalLocation}...`);
                const API_URL = `https://api.weatherapi.com/v1/current.json?key=${WEATHER_API_KEY}&q=${finalLocation}`;
                const response = await fetch(API_URL);
                const data = await response.json();

                if (response.ok && data.location) {
                    const summary = purpose === 'time'
                        ? `The current time in ${data.location.name} is ${new Date(data.location.localtime).toLocaleTimeString('en-US', { hour: 'numeric', minute: 'numeric', hour12: true })}.`
                        : `The weather in ${data.location.name} is ${Math.round(data.current.temp_c)} degrees with ${data.current.condition.text}.`;
                    await speakAndPlay(summary);
                } else {
                    throw new Error(data.error?.message || "Invalid city data.");
                }
            } catch (error) {
                console.error("ðŸ”´ Weather/Location Error:", error);
                await speakAndPlay(`Sorry, I couldn't get the information for that location.`);
            }
        }

        // --- ðŸ”Š TTS & PLAYBACK FUNCTION using Web Speech API ---
        async function speakAndPlay(text) {
            if (!text || !text.trim() || !synthesis) {
                 setUIState('idle');
                 return;
            }
            if (synthesis.speaking) synthesis.cancel();

            return new Promise((resolve, reject) => {
                setUIState('speaking');
                const utterance = new SpeechSynthesisUtterance(text);
                if (preferredVoice) utterance.voice = preferredVoice;
                utterance.pitch = 1;
                utterance.rate = 1;

                utterance.onend = () => {
                    setUIState('idle');
                    resolve();
                };
                utterance.onerror = (event) => {
                    console.error("ðŸ”´ Web Speech API Error:", event.error);
                    setUIState('error', 'Voice synthesis failed.');
                    reject(event.error);
                };
                synthesis.speak(utterance);
            });
        }
    </script>

</body>
</html>
