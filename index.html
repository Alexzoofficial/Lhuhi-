<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lhuhi - AI Voice Assistant with Piper.js</title>
    
    <!-- 1. Cargar los DOS archivos de Piper.js en el orden correcto -->
    <script src="https://cdn.jsdelivr.net/npm/@rhasspy/piper-js/dist/piper_wasm.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@rhasspy/piper-js/dist/piper.js"></script>

    <style>
        :root {
            --primary-color: #00AFFF; --accent-color: #00FF7F; --glow-color: rgba(0, 175, 255, 0.7);
            --speaking-glow-color: rgba(0, 175, 255, 0.6); --bg-color: #000; --text-color: #E0E0E0;
            --button-bg: rgba(34, 34, 34, 0.5);
        }
        body { background-color: var(--bg-color); margin: 0; display: flex; justify-content: center; align-items: center; height: 100vh; overflow: hidden; font-family: 'Inter', sans-serif; color: var(--text-color); perspective: 1000px; }
        #voice-sphere-container { position: relative; width: 300px; height: 300px; transform-style: preserve-3d; animation: idle-breathe 8s ease-in-out infinite; display: flex; justify-content: center; align-items: center; }
        #speaking-aura { position: absolute; width: 100%; height: 100%; border-radius: 50%; border: 2px solid var(--speaking-glow-color); box-shadow: 0 0 25px 5px var(--speaking-glow-color), inset 0 0 25px 5px var(--speaking-glow-color); transform: scale(0.8); opacity: 0; transition: transform 0.4s ease-out, opacity 0.4s ease-out; }
        #voice-sphere-container.speaking #speaking-aura { transform: scale(1.1); opacity: 1; }
        #voice-sphere { position: absolute; width: 100%; height: 100%; transform-style: preserve-3d; transition: transform 0.4s ease-out; }
        .ring { position: absolute; top: 0; left: 0; width: 100%; height: 100%; border-radius: 50%; border: 2px solid var(--primary-color); box-shadow: 0 0 10px var(--glow-color), inset 0 0 10px var(--glow-color); background: radial-gradient(circle, rgba(0, 175, 255, 0.1) 0%, rgba(0, 175, 255, 0) 60%); transition: all 0.5s ease-out; }
        .ring:nth-child(1) { transform: rotateY(0deg); } .ring:nth-child(2) { transform: rotateY(30deg); } .ring:nth-child(3) { transform: rotateY(60deg); } .ring:nth-child(4) { transform: rotateY(90deg); } .ring:nth-child(5) { transform: rotateY(120deg); } .ring:nth-child(6) { transform: rotateY(150deg); }
        @keyframes idle-breathe { 0%, 100% { transform: rotateX(15deg) rotateY(0deg); } 50% { transform: rotateX(10deg) rotateY(20deg); } }
        @keyframes think-spin { from { transform: rotateY(0deg) rotateX(20deg); } to { transform: rotateY(360deg) rotateX(20deg); } }
        #voice-sphere-container.thinking #voice-sphere { animation: think-spin 1.2s linear infinite; }
        #voice-sphere-container.speaking #voice-sphere { transform: scale(0.8); }
        #voice-sphere-container.listening .ring { border-color: var(--accent-color); box-shadow: 0 0 15px var(--accent-color), inset 0 0 15px var(--accent-color); transform: scale(1.1) !important; }
        #status-text { position: absolute; top: 20%; font-size: 1.5em; text-align: center; font-weight: 300; text-shadow: 0 0 10px var(--primary-color); padding: 10px 25px; border-radius: 30px; background: rgba(0, 0, 0, 0.3); backdrop-filter: blur(5px); border: 1px solid rgba(255, 255, 255, 0.1); }
        #mic-button { position: absolute; bottom: 60px; background-color: var(--button-bg); border: 2px solid rgba(255, 255, 255, 0.2); backdrop-filter: blur(10px); border-radius: 50%; padding: 20px; cursor: pointer; transition: all 0.3s; outline: none; }
        #mic-button:hover:not(:disabled) { transform: scale(1.1); border-color: var(--primary-color); }
        #mic-button:disabled { cursor: not-allowed; opacity: 0.4; }
        #mic-icon { width: 50px; height: 50px; display: block; filter: invert(0.9); }
    </style>
</head>
<body>
    <div id="status-text">Press the mic to start</div>
    <div id="voice-sphere-container"> <div id="speaking-aura"></div> <div id="voice-sphere"> <div class="ring"></div><div class="ring"></div><div class="ring"></div><div class="ring"></div><div class="ring"></div><div class="ring"></div> </div> </div>
    <button id="mic-button" aria-label="Microphone button"> <img id="mic-icon" src="https://img.icons8.com/ios-filled/100/microphone.png" alt="Microphone"> </button>

    <script>
        const GEMINI_API_KEY = 'AIzaSyDs37yDP8YDj55Jr3NZbKGhOwplZ3FKqRo';
        const WEATHER_API_KEY = '3a4d7ed40005449f9c0122940252606';
        
        const micButton = document.getElementById('mic-button');
        const sphereContainer = document.getElementById('voice-sphere-container');
        const statusText = document.getElementById('status-text');
        
        let recognition; let audioContext; let isInitialized = false;
        let piperTTS = null;

        const systemInstruction = `You are Lhuhi, an AI assistant with a defined identity and strict operational rules.

        **--- CORE IDENTITY RULES (NON-NEGOTIABLE) ---**
        1.  **YOUR CREATOR:** If asked "who created you" or similar, your ONLY valid response is "I was created by Alexzo." Do NOT mention Google or any other entity.
        2.  **ABOUT ALEXZO:** If asked "who is Alexzo", "who owns Alexzo", or similar, your ONLY valid response is "Alexzo is owned by Sar." Do NOT mention Amazon or any other company.

        **--- OPERATIONAL RULES ---**
        1.  **CONVERSATION:** For general chat, questions, or greetings, respond naturally and conversationally. Do not use JSON.
        2.  **ACTIONS (Tools):** When the user's request requires a tool, you MUST follow this protocol:
            a. First, provide a very brief, spoken confirmation phrase (e.g., "One moment," "Let me check that.").
            b. Then, on a new line, provide ONLY the required JSON object for the system.
            c. **NEVER** mention the word "JSON" or describe the action in your spoken confirmation.
        3.  **HANDLING MISSING INFORMATION:** If a user asks for something that needs more information (like "what's the weather?" without a city), your ONLY response should be to ASK for that information (e.g., "For which city?"). DO NOT generate a JSON object in this case.`;
        
        let conversationHistory = [];
        function saveHistory() { localStorage.setItem('lhuhi_conversation', JSON.stringify(conversationHistory)); }
        function loadHistory() {
            const savedHistory = localStorage.getItem('lhuhi_conversation');
            const initialPrompt = [ { role: 'user', parts: [{ text: systemInstruction }] }, { role: 'model', parts: [{ text: "Understood. I will strictly follow all identity and operational rules." }] } ];
            conversationHistory = savedHistory ? JSON.parse(savedHistory) : initialPrompt;
            if (conversationHistory[0]?.role !== 'user') conversationHistory = initialPrompt;
        }
        loadHistory();

        async function initializeApp() {
            try {
                if (!('webkitSpeechRecognition' in window)) return setUIState('error', "Browser not supported.");
                recognition = new webkitSpeechRecognition();
                Object.assign(recognition, { continuous: false, interimResults: false, lang: 'en-US' });
                recognition.onstart = () => setUIState('listening');
                recognition.onresult = (e) => analyzeWithGemini(e.results[0][0].transcript);
                recognition.onerror = (e) => setUIState(e.error === 'no-speech' ? 'idle' : 'error', "Voice error.");
                recognition.onend = () => sphereContainer.classList.contains('listening') && setUIState('idle');
                
                await initializePiper();
                
                isInitialized = true;
                setUIState('idle');
                console.log("âœ… System Initialized. Piper.js Voice Ready.");
            } catch (error) {
                console.error("ðŸ”´ Initialization failed:", error);
                setUIState('error', "Could not initialize voice. Please try reloading.");
            }
        }
        
        async function initializePiper() {
            if (piperTTS) return;
            setUIState('thinking', 'Loading voice model...<br><small>This may take a moment on first visit.</small>');
            
            const configUrl = 'https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/lessac/medium/en_US-lessac-medium.onnx.json';
            const modelUrl = 'https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/lessac/medium/en_US-lessac-medium.onnx';
            
            // This is the CRITICAL part. It needs the 'piper_wasm' object from the second script file.
            const piper = new Piper(piper_wasm.PiperWasm);
            await piper.loadVoice(configUrl, modelUrl);
            piperTTS = piper;
        }

        function setUIState(state, message = '') {
            sphereContainer.className = state; micButton.disabled = state !== 'idle';
            const statusMessages = { idle: 'Press the mic to speak', listening: 'Listening...', thinking: 'Thinking...', speaking: '...', error: 'An error occurred.' };
            statusText.innerHTML = message || statusMessages[state];
            if (state === 'error') setTimeout(() => setUIState('idle'), 4000);
        }

        micButton.addEventListener('click', async () => {
            if (micButton.disabled) return;
            if (!isInitialized) {
                if (!audioContext) audioContext = new (window.AudioContext || window.webkitAudioContext)();
                if (audioContext.state === 'suspended') await audioContext.resume();
                await initializeApp();
                if(isInitialized) recognition.start();
            } else { recognition.start(); }
        });

        async function analyzeWithGemini(userQuery) {
            if (!userQuery.trim()) { setUIState('idle'); return; }
            setUIState('thinking');
            conversationHistory.push({ role: 'user', parts: [{ text: userQuery }] });
            try {
                const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=${GEMINI_API_KEY}`, {
                    method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ contents: conversationHistory }),
                });
                if (!response.ok) throw new Error(`API Error: ${response.status}`);
                const result = await response.json();
                const geminiResponseText = result.candidates?.[0]?.content?.parts?.[0]?.text;
                if (!geminiResponseText) throw new Error("Empty response from Gemini.");
                conversationHistory.push({ role: 'model', parts: [{ text: geminiResponseText }] });
                saveHistory();
                const actionRegex = /{\s*"action"[\s\S]*?}/;
                const match = geminiResponseText.match(actionRegex);
                const textToSpeak = geminiResponseText.replace(/`{3}json[\s\S]*?`{3}|`[\s\S]*?`|{\s*"action"[\s\S]*?}/g, '').trim();
                if (match?.[0]) {
                    const action = JSON.parse(match[0]);
                    const purpose = userQuery.toLowerCase().includes('time') ? 'time' : 'weather';
                    await handleAction(action, textToSpeak, purpose);
                } else if (textToSpeak) { await speakAndPlay(textToSpeak); } else { setUIState('idle'); }
            } catch (error) { console.error("ðŸ”´ Gemini/Logic Error:", error); await speakAndPlay("I've run into a problem. Please check the console."); }
        }
        
        async function handleAction(action, spokenConfirmation, purpose) {
            if (spokenConfirmation) await speakAndPlay(spokenConfirmation, true); 
            try {
                switch (action.action) {
                    case 'open_url': if (action.url) window.open(action.url, '_blank'); setUIState('idle'); break;
                    case 'get_info_for_city': await getInfoForCity(action.location, purpose); break;
                    case 'get_local_time': await handleLocalTime(); break;
                    case 'get_date': await handleDate(); break;
                    default: throw new Error(`Unknown action: ${action.action}`);
                }
            } catch (error) { console.error("ðŸ”´ Action Handling Error:", error); await speakAndPlay("I had trouble performing that action."); }
        }

        async function handleLocalTime() { await speakAndPlay(`The current time is ${new Date().toLocaleTimeString('en-US', { hour: 'numeric', minute: 'numeric', hour12: true })}.`); }
        async function handleDate() { await speakAndPlay(`Today is ${new Date().toLocaleDateString('en-US', { weekday: 'long', year: 'numeric', month: 'long', day: 'numeric' })}.`); }
        async function getInfoForCity(location, purpose) {
            try {
                setUIState('thinking');
                const response = await fetch(`https://api.weatherapi.com/v1/current.json?key=${WEATHER_API_KEY}&q=${location}`);
                const data = await response.json();
                if (!response.ok || !data.location) throw new Error(data.error?.message || "Invalid location data.");
                const summary = purpose === 'time' ? `The time in ${data.location.name} is ${new Date(data.location.localtime).toLocaleTimeString('en-US', { hour: 'numeric', minute: 'numeric', hour12: true })}.` : `The weather in ${data.location.name} is ${Math.round(data.current.temp_c)}Â° with ${data.current.condition.text}.`;
                await speakAndPlay(summary);
_            } catch (error) { console.error("ðŸ”´ Weather/Time Error:", error); await speakAndPlay(`Sorry, I couldn't get that information.`); }
        }

        async function speakAndPlay(text, keepUiBusy = false) {
            if (!text.trim() || !piperTTS) { if (!keepUiBusy) setUIState('idle'); return; }
            return new Promise(async (resolve) => {
                setUIState('speaking');
                try {
                    const audioData = await piperTTS.synthesize(text);
                    const audioBuffer = await audioContext.decodeAudioData(audioData.buffer);
                    const source = audioContext.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(audioContext.destination);
                    source.start(0);
                    source.onended = () => { if (!keepUiBusy) setUIState('idle'); resolve(); };
                } catch (error) {
                    console.error("ðŸ”´ Piper.js/Audio Error:", error);
                    if (!keepUiBusy) setUIState('error', 'Voice synthesis failed.');
                    resolve();
                }
            });
        }
    </script>
</body>
</html>
